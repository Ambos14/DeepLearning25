{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_CIFAR10.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Vision Transformers on CIFAR10"
      ],
      "metadata": {
        "id": "9rZPsOU0phDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i0AIosM0AaY2"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "dataset = dset.CIFAR10(root=\"./data\", download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "nc=3\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "is2FpH_lAtJJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the availability of cuda devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "yF-tedlhAx-s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks:\n",
        "* try to get the best test Accuracy on Cifar10 using a transformer model\n",
        "* pre-trained models allowed - see [here](https://docs.pytorch.org/vision/main/models/vision_transformer.html) for list of models in TorchVision\n",
        "* **hint**: just like with the CNN in Week 5 - wee need to change the classification layer to fit our 10 class CIFAR-10 problem before we can fine-tune it...\n",
        "* **hint**: Transformers need a lot of compute + memory - use the A100 GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "KxRD7Myvpogs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install -q timm pandas requests"
      ],
      "metadata": {
        "id": "gMhxLnIeBfBe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from timm.models import create_model\n",
        "\n",
        "# Modell definieren (mit Upsample-Schicht vorne)\n",
        "class DeiTWithUpsample(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        self.model = create_model('deit_base_patch16_224', pretrained=True)\n",
        "        self.model.head = nn.Linear(self.model.head.in_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m_Dbc3R6vENq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeiTWithUpsample().to(device)\n",
        "\n",
        "# Daten ohne Resize laden\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Loss und Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "XjB9mxA4-HoD",
        "outputId": "288c4110-1c13-4a56-b74a-21e9ecfcfca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    train_acc = 100. * correct / total\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "\n",
        "print(\"Training abgeschlossen.\")\n"
      ],
      "metadata": {
        "id": "3D83Dlut-K9J",
        "outputId": "140cbd6f-a0d4-44ca-b13c-1d396b0b44a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.1020, Train Acc: 96.74%\n",
            "Epoch 2/5, Train Loss: 0.0463, Train Acc: 98.47%\n",
            "Epoch 3/5, Train Loss: 0.0358, Train Acc: 98.79%\n",
            "Epoch 4/5, Train Loss: 0.0289, Train Acc: 99.07%\n",
            "Epoch 5/5, Train Loss: 0.0262, Train Acc: 99.08%\n",
            "Training abgeschlossen.\n"
          ]
        }
      ]
    }
  ]
}